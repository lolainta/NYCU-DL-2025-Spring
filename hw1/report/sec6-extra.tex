\subsection{Different Optimizers}

TBD

\subsection{Different Activation Functions}

For this experiment, we will compare the performance of different activation on both the linear and XOR datasets.
The activation functions we will compare are:
\begin{itemize}
    \item Sigmoid
    \item ReLU
    \item Tanh
    \item Identity (No activation function)
\end{itemize}

The experimental results for sigmoid and Identity are shown previously in Table \ref{tab:comparison_matrix_sigmoid} and Table \ref{tab:comparison_matrix_none}.

The experimental results for ReLU and Tanh are shown in Table \ref{tab:comparison_matrix_relu} and Table \ref{tab:comparison_matrix_tanh}.

\comparisontable{relu}
\comparisontable{tanh}

\subsection{Implement Convolutional Layers}
