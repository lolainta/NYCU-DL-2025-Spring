\section{Implementation Details}
\label{sec:implementation}

\subsection{Refactoring the Sample Code}

First, since the sample code is not modular, we refactored the code into multiple classes. The main classes are:
\begin{itemize}
      \item \textbf{DQN}: This class implements the neural network architecture. It is a subclass of the \href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module}{\texttt{torch.nn.Module}} class.
      \item \textbf{DQNAgent}: This class represents the agent that interacts with the environment. It contains methods for selecting actions, storing experiences, and updating the Q-values.
            The agent is responsible for the exploration and exploitation of the environment.
            It also contains the experience replay buffer, which stores the agent's experiences for training.
            However, the backend neural network is modularized into a separate class. That is we can reuse the same agent class for different neural networks dealing with different environments we focused on.
      \item \textbf{Trainer}: This class is responsible for training the agent. It handles the training loop, including collecting experiences, updating the model, and logging results.
            By the way, the trainer class is also responsible for loading the environment and the agent. It will handle agent's actions and interact it with the environment.
      \item \textbf{Tester}: This class is responsible for testing the agent. It handles the testing loop, it is similar to the trainer class, but it does not update the model.
            The tester class is also responsible for loading the environment and the agent. It will handle agent's actions and interact it with the environment.
\end{itemize}

With this refactoring, I can easily add new features and techniques to the agent, trainer, and tester classes without modifying the core logic of the DQN algorithm.
This modularity also allows us to reuse the same code for different tasks and environments, making it easier to experiment with different architectures and techniques.

\subsubsection{DQN Class}

The DQN class is a subclass of the \href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module}{\texttt{torch.nn.Module}} class. It implements the neural network architecture for the DQN algorithm.

For the \cartpole environment, I used a simple feedforward neural network with two hidden layers. The input layer has 4 neurons (one for each state variable), and the output layer has 2 neurons (one for each action).
Both hidden layers have 128 neurons, and the activation function is ReLU.
The architecture is designed to be simple and efficient, as the \cartpole environment has a relatively low-dimensional state space.
The output layer uses a linear activation function, which is suitable for the DQN algorithm since it outputs Q-values for each action.

The implementation of the \texttt{CartPoleDQN} class is as follows:
\inputminted[firstline=5,lastline=17]{python}{../models.py}

For the \pong environment, I used a convolutional neural network (CNN) with three convolutional layers and two fully connected layers. The architecture is designed to effectively capture the spatial features of the input frames, allowing the agent to learn optimal policies for playing the game.

The implementation of the \texttt{PongDQN} class is as follows:
\inputminted[firstline=20]{python}{../models.py}

\subsubsection{DQNAgent Class}

The DQNAgent class is responsible for interacting with the environment and managing the agent's experiences.
It contains methods for selecting actions, storing experiences, and updating the Q-values.

The agent uses an $\epsilon$-greedy policy for action selection, where it explores the environment with a probability of $\epsilon$ and exploits the learned Q-values with a probability of 1 - $\epsilon$.
The agent also maintains a replay buffer, which stores the agent's experiences for training.
The replay buffer is implemented using the the \texttt{PrioritizedReplayBuffer} class, which will be discussed in Section \ref{sec:per}.

The implementation of the \texttt{DQNAgent} class is as follows:
\inputminted[firstline=85,highlightlines={102,136}]{python}{../dqn.py}

\subsubsection{Trainer Class}

The Trainer class is responsible for training the agent.
It handles the training loop, including collecting experiences, updating the model, and logging results.
The trainer class is also responsible for loading the environment and the agent. It will handle agent's actions and interact it with the environment.
The implementation of the \texttt{Trainer} class is as follows:
\inputminted[firstline=18,lastline=176,highlightlines={44,60,123}]{python}{../trainer.py}

\subsubsection{Tester Class}

The Tester class is responsible for testing the agent.
It handles the testing loop, it is similar to the trainer class, but it does not update the model.
The tester class is also responsible for loading the environment and the agent.
It will handle agent's actions and interact it with the environment.
The implementation of the \texttt{Tester} class is as follows:
\inputminted[firstline=19,lastline=102,highlightlines={39,82}]{python}{../tester.py}

\subsection{Hyperparameters}

The hyperparameters used in the training process are crucial for the performance of the DQN algorithm.

For Tasks 1 and 2, we are required to use vanilla DQN without any modifications.
To meet this requirement, certain hyperparameters must be set accordingly.
For example, setting $\alpha=0$ in the \texttt{PrioritizedReplayBuffer} makes the buffer behave like a uniform replay buffer.
Additionally, setting \texttt{update\_period = 1} in the DQNAgent class ensures that the model is updated every time a batch is sampled from the replay buffer, aligning with the standard behavior of vanilla DQN.
And last, setting \texttt{n\_steps=1} in the DQNAgent class ensures that the agent uses one-step returns for training, which is standard in vanilla DQN.


\subsection{Bellman Equation}

The Bellman equation is a fundamental concept in reinforcement learning that describes the relationship between the value of a state and the values of its successor states.
In the context of DQN, the Bellman equation is used to update the Q-values based on the agent's experiences.
The Bellman equation for Q-learning is given by:
\begin{equation}
      Q(s, a) = r + \gamma \max_{a'} Q(s', a')
\end{equation}
where:
\begin{itemize}
      \item $Q(s, a)$ is the Q-value for state $s$ and action $a$.
      \item $r$ is the reward received after taking action $a$ in state $s$.
      \item $\gamma$ is the discount factor, which determines the importance of future rewards.
      \item $s'$ is the next state after taking action $a$ in state $s$.
      \item $\max_{a'} Q(s', a')$ is the maximum Q-value for the next state $s'$ over all possible actions $a'$.
\end{itemize}
The DQN algorithm uses a neural network to approximate the Q-values, and the Bellman equation is used to update the weights of the network during training.
The Q-value update is performed using the following loss function:
\begin{equation}
      L(\theta) = \mathbb{E}_{(s, a, r, s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
\end{equation}
where:
\begin{itemize}
      \item $L(\theta)$ is the loss function.
      \item $\theta$ are the weights of the Q-network.
      \item $\theta^-$ are the weights of the target network, which are updated periodically.
      \item $D$ is the replay buffer containing the agent's experiences.
      \item $\mathbb{E}$ is the expectation operator, which averages over the experiences in the replay buffer.
\end{itemize}
The loss function measures the difference between the predicted Q-value and the target Q-value. The weights of the Q-network are updated using gradient descent to minimize this loss.
The target Q-value is computed using the Bellman equation, and the target network is used to stabilize the training process.
The target network is a separate copy of the Q-network, updated periodically every $C$ steps (a hyperparameter) by copying the weights of the Q-network.
This periodic update reduces the variance of the Q-value updates, making the training process more stable and efficient.

The implementation of the Bellman equation in the DQN algorithm is done in the \texttt{learn} method of the \texttt{DQNAgent} class.
In this method, the Q-values are updated based on the agent's experiences, ensuring that the learning process aligns with the principles of reinforcement learning.
The implementation of the \texttt{learn} method is as follows:

\inputminted[firstline=136, highlightlines={155-158}]{python}{../dqn.py}

\subsection{Prioritized Experience Replay}
\label{sec:per}

\subsection{Multi-Step Reward}

The multi-step reward is a technique used in reinforcement learning to improve the efficiency of learning by considering multiple steps of experience at once.
In the context of DQN, multi-step rewards are used to update the Q-values based on a sequence of actions and rewards, rather than just the immediate reward.
This approach allows the agent to learn from longer-term dependencies and can lead to faster convergence and better performance.
The multi-step reward is computed by summing the rewards over a sequence of $n$ steps, discounted by the discount factor $\gamma$:
\begin{equation}
      R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots + \gamma^{n-1} r_{t+n-1}
\end{equation}
where:
\begin{itemize}
      \item $R_t$ is the multi-step reward at time step $t$.
      \item $r_t$ is the immediate reward at time step $t$.
      \item $\gamma$ is the discount factor.
      \item $n$ is the number of steps to consider for the multi-step reward.
      \item $r_{t+k}$ is the immediate reward at time step $t+k$.
\end{itemize}
The multi-step reward is used to update the Q-values in the same way as the standard Bellman equation, but it incorporates the rewards from multiple steps.
This allows the agent to learn from longer-term dependencies and can lead to faster convergence and better performance.
The multi-step reward is implemented in the \texttt{PrioritizedReplayBuffer} class.

\inputminted[firstline=20, lastline=82,highlightlines={43,53-60,72-75}]{python}{../dqn.py}

\subsection{Weight \& Biases Using Techniques}

Since we are required to complete 3 different tasks in two different environments, I used the \href{https://wandb.ai/site}{Weight \& Biases} (WandB) library to track the training process and visualize the results.

\subsubsection{Categorizing the Results}
To categorize the results, I used the \texttt{wandb.init} method to create a new run for each task and add tags to the run.
This allows me to easily filter and compare the results of different runs in the WandB dashboard for the same task.

\subsubsection{Snapshotting the Code}
I also used set argument \texttt{save\_code} to \texttt{True} and setting the \texttt{code\_dir} argument to the whole current directory to save all \texttt{.py} files in the WandB run.
This allows me to easily reproduce the results and compare the code used for different runs.

\subsubsection{Hyperparameter Tuning}
I used the \texttt{wandb.config} method to define the hyperparameters for each run.
This allows me to easily track and compare the hyperparameters used for different runs in the WandB dashboard.

\subsubsection{Result Plot Generation}
I used the \texttt{wandb.log} method to log the results of each run.
This allows me to easily visualize the results in the WandB dashboard and compare the performance of different runs.
This also allows me to easily export the plots and use them in the report.

\subsubsection{WandB Initialization}
The implementation of the WandB initialization in the \texttt{Trainer} class is as follows:
\inputminted[firstline=241,lastline=248,highlightlines={244-246,248}]{python}{../trainer.py}
And the implementation of the WandB initialization in the \texttt{Tester} class is as follows:
\inputminted[firstline=152,lastline=158,highlightlines={154-156,158}]{python}{../tester.py}
